import streamlit as st
import re
import requests
import pandas as pd
from src.mediawiki_uploader import upload_to_bahaiworks
from src.sitelink_manager import set_sitelink
from src.wikibase_importer import get_or_create_author

st.set_page_config(
    page_title="Bahai.works Utilities",
    layout="wide"
)

st.title("üõ†Ô∏è Miscellaneous Utilities")

# --- HELP SECTION ---
with st.expander("‚ÑπÔ∏è Help / Instructions"):
    st.markdown("""
    **1. Create Author Pages**
    * **Batch Creation:** Paste a list of author names (comma-separated) to create their pages in bulk.
    * **Modes:**
        * *Dynamic:* Uses a Lua module to automatically list all chapters by this author in the works of others.
        * *Static:* Hard-codes a link to a specific book. Use this for authors with a single major work.
    * **Bahaidata:** This tool automatically creates the Bahaidata item for the author (if missing) and links it to the new Bahai.works page.

    **2. AC Messages**
    * **Copyright AC-Message:** Generates the special `/AC-Message` subpage required for copyright-protected books.
    * **Usage:** Enter the exact Book Title (Page Name) and the cover image filename.

    **3. Update Author list**
    * **Update Authors Index:** Scans all `Category:Authors-A` through `Z` and rebuilds the main [[Authors]] index page. Run this periodically if the main index feels out of date.
    """)

st.markdown("---")

# --- TABS ---
tab_create_author, tab_ac, tab_update_list, tab_maintenance = st.tabs([
    "üë§ Create Author Pages", 
    "üìñ AC Messages", 
    "üìù Update Author list",
    "üîß Maintenance"
])

# --- Author page maintenance and exclusions ---
AUTHORS_PAGE_HEADER = """{{header
 | title      = Authors
 | author     =
 | translator =
 | section    = 
 | previous   = 
 | next       = 
 | notes      = Provided below is an incomplete, but growing list of Bah√°‚Äô√≠ authors and their works. Also included are authors who have published articles in books or periodicals such as [[World Order]]. 
}}
{| class="wikitable" style="float:right; margin-left: 10px;"
! Number of listed authors
|-
| style="text-align:center;" | {{#expr: {{PAGESINCATEGORY:Authors-A}} + {{PAGESINCATEGORY:Authors-B}} + {{PAGESINCATEGORY:Authors-C}} + {{PAGESINCATEGORY:Authors-D}} + {{PAGESINCATEGORY:Authors-E}} + {{PAGESINCATEGORY:Authors-F}} + {{PAGESINCATEGORY:Authors-G}} + {{PAGESINCATEGORY:Authors-H}} + {{PAGESINCATEGORY:Authors-I}} + {{PAGESINCATEGORY:Authors-J}} + {{PAGESINCATEGORY:Authors-K}} + {{PAGESINCATEGORY:Authors-L}} + {{PAGESINCATEGORY:Authors-M}} + {{PAGESINCATEGORY:Authors-N}} + {{PAGESINCATEGORY:Authors-O}} + {{PAGESINCATEGORY:Authors-P}} + {{PAGESINCATEGORY:Authors-Q}} + {{PAGESINCATEGORY:Authors-R}} + {{PAGESINCATEGORY:Authors-S}} + {{PAGESINCATEGORY:Authors-T}} + {{PAGESINCATEGORY:Authors-U}} + {{PAGESINCATEGORY:Authors-V}} + {{PAGESINCATEGORY:Authors-W}} + {{PAGESINCATEGORY:Authors-X}} + {{PAGESINCATEGORY:Authors-Y}} + {{PAGESINCATEGORY:Authors-Z}} }}
|}
* [[Author:Bah√°‚Äôu‚Äôll√°h|Bah√°‚Äôu‚Äôll√°h]]
* [[Author:The B√°b|The B√°b]]
* [[Author:‚ÄòAbdu‚Äôl-Bah√°|‚ÄòAbdu‚Äôl-Bah√°]]
* [[Author:Shoghi Effendi|Shoghi Effendi]]
* [[Author:Universal House of Justice|Universal House of Justice]]
* [[Author:Institutional authors|Other institutional authors]]


{{CompactTOC}}
"""

EXCLUSION_LIST = [
    "Author:‚ÄòAbdu‚Äôl-Bah√°", "Author:Association for Bah√°‚Äô√≠ Studies North America", "Author:The B√°b",
    "Author:Bah√°‚Äôu‚Äôll√°h", "Author:Bah√°‚Äô√≠ Canada Publications", "Author:Bah√°‚Äô√≠ International Community",
    "Author:Bah√°‚Äô√≠ Publishing Trust, United States", "Author:Bah√°‚Äô√≠ World Centre",
    "Author:Canadian Bah√°‚Äô√≠ Community National Office", "Author:Child Education Committee",
    "Author:Shoghi Effendi", "Author:Steve Gregg", "Author:Marriage and Family Development Committee",
    "Author:National Bah√°‚Äô√≠ Women‚Äôs group", "Author:National Bah√°‚Äô√≠ Youth Committee, United States",
    "Author:National Community Development Committee", "Author:National Education Committee, United States",
    "Author:National Programming Committee", "Author:National Reference Library Committee",
    "Author:NSA, United Kingdom", "Author:National Spiritual Assembly of the United States",
    "Author:National Spiritual Assembly of the British Isles", "Author:National Teaching Committee, United States",
    "Author:Office of Public Information", "Author:Office of Social and Economic Development",
    "Author:Office of the Treasurer, United States", "Author:Charles Mason Remey", "Author:Research Department",
    "Author:Study Aids Committee, United States", "Author:Study Outline Committee, United States",
    "Author:NSA, Australia", "Author:NSA, British Isles", "Author:NSA, United States",
    "Author:NSA, United States and Canada", "Author:NSA, Iran", "Author:NSA, South Africa",
    "Author:Universal House of Justice"
]

# --- HELPER FUNCTIONS ---

def get_lastname_firstname(full_name):
    """
    Parses names into 'Lastname, Firstname' format.
    
    1. Handles suffixes (Jr., Sr., III) -> 'Trafton, Jr., Burton'
    2. Handles particles (de, van, von) -> 'de Araujo, Victor'
    """
    # 1. Clean and Split
    name = full_name.strip()
    parts = name.split()
    
    if len(parts) <= 1:
        return name

    # 2. Extract Suffixes (Case-insensitive check)
    suffixes = ['jr', 'jr.', 'sr', 'sr.', 'ii', 'iii', 'iv', 'v', 'vi']
    suffix = ""
    
    # Check the last word to see if it's a suffix
    last_word_norm = parts[-1].lower().replace(',', '').replace('.', '')
    
    # We strip '.' from the check list above to match "Jr." or "Jr"
    if last_word_norm in [s.replace('.', '') for s in suffixes]:
        suffix = parts[-1].replace(',', '') # Store the suffix (e.g. "Jr.")
        parts = parts[:-1] # Remove suffix from the working list
        # Clean any trailing comma from the new last word (e.g., "Gulick," -> "Gulick")
        parts[-1] = parts[-1].rstrip(',')

    # 3. Handle Connectors/Particles
    # These are words that signal the start of a Last Name
    connectors = {"de", "dos", "da", "do", "von", "van", "den", "der"}
    
    split_index = -1
    
    # Find the *first* occurrence of a connector to start the Last Name there
    for i, part in enumerate(parts):
        # We generally assume a connector won't be the very first name (Index 0)
        # unless it's a mononym, but for safety we check i > 0 usually.
        # However, "De Man" is a valid name. Let's stick to your logic:
        if part.lower() in connectors:
            split_index = i
            break
    
    if split_index > 0: 
        # Case: "Victor [de] Araujo"
        firstname_part = " ".join(parts[:split_index])
        lastname_part = " ".join(parts[split_index:])
    else:
        # Standard Case: Split at the very last word
        firstname_part = " ".join(parts[:-1])
        lastname_part = parts[-1]

    # 4. Final formatting
    if suffix:
        # Format: Lastname, Suffix, Firstname
        return f"{lastname_part}, {suffix}, {firstname_part}"
    else:
        # Format: Lastname, Firstname
        return f"{lastname_part}, {firstname_part}"

def format_author_page(name, book_title=None, book_year=None, use_dynamic=True):
    """
    Generates the content for the Author:Name page.
    If use_dynamic is True, it uses the Lua module.
    Otherwise, it links the specific book provided.
    """
    content = "{{author2}}\n\n===Publications===\n"
    
    if use_dynamic:
        content += "==== Contributing author====\n{{#invoke:Chapters|getChaptersByAuthor}}\n"
    else:
        if book_title and book_year:
            content += f"[[{book_title}]] ({book_year})\n"
        elif book_title:
             content += f"[[{book_title}]]\n"
             
    content += "\n__NOTOC__"
    return content

def format_author_cat_page(name):
    """Generates content for Category:Name"""
    sort_key = get_lastname_firstname(name)
    return f"{{{{authorcat_desc}}}}\n[[Category:Authors|{sort_key}]]"

def format_works_cat_page(name):
    """Generates content for Category:Text_of_works_by_Name"""
    return f"{{{{Textof_desc}}}}\n[[Category:{name}]]"

def format_ac_message(title, cover_file):
    return f"""{{{{AC-Template
| title    = {title}
| cover    = {cover_file}
}}}}"""

def ensure_wikibase_author(author_name):
    """
    1. Gets/Creates Wikibase Item (using clean src logic).
    2. Links it to 'Author:Name'.
    """
    # 1. Get QID (Creates if missing)
    qid = get_or_create_author(author_name)
    
    if not qid:
        return None, False, "Failed to retrieve QID"

    # 2. Link to Bahai.works
    # Ensure target page format is "Author:Name"
    if author_name.startswith("Author:"):
        target_page = author_name
    else:
        target_page = f"Author:{author_name}"
        
    success, msg = set_sitelink(qid, target_page)
    
    return qid, success, msg

# --- TAB: CREATE AUTHOR PAGES ---

with tab_create_author:
    st.header("Create Author Pages")
    st.info("This tool creates the three required pages (Author, Category, Works Category) for each author.")

    # Define a specific key for the text area so we can manipulate it
    text_area_key = "author_input_area"

    # 1. Handle Incoming Data from Chapter Items
    if "batch_author_list" in st.session_state:
        # Convert list to comma-separated string
        imported_authors = ", ".join(st.session_state["batch_author_list"])
        
        # FORCE update the widget's state directly
        st.session_state[text_area_key] = imported_authors
        
        st.success(f"üì• Received {len(st.session_state['batch_author_list'])} missing authors from Chapter Manager.")
        
        # Cleanup the transfer variable
        del st.session_state["batch_author_list"]

    # Ensure the key exists in session state to avoid KeyErrors
    if text_area_key not in st.session_state:
        st.session_state[text_area_key] = ""

    c1, c2 = st.columns(2)
    
    with c1:
        # Because 'key' is set, it will automatically read from st.session_state[text_area_key]
        raw_authors = st.text_area(
            "Author Names (comma separated)", 
            key=text_area_key,
            placeholder="e.g. Aaron Emmel, John Doe, Jane Smith",
            height=150
        )
        
        # Options for the Author Page content
        st.subheader("Configuration")
        content_mode = st.radio("Publications List Style", ["Dynamic (Lua Module)", "Static (Specific Book)"])
        
        book_title = ""
        book_year = ""
        
        if content_mode == "Static (Specific Book)":
            book_title = st.text_input("Book Title", placeholder="e.g. The Bah√°'√≠ Faith")
            book_year = st.text_input("Year", placeholder="e.g. 1919")

    with c2:
        st.subheader("Preview & Execute")
        
        # Parse Input
        if raw_authors:
            # Split by comma, strip whitespace, remove empty strings
            author_list = [name.strip() for name in raw_authors.split(",") if name.strip()]
            
            if not author_list:
                st.warning("Please enter valid author names.")
            else:
                st.write(f"**Found {len(author_list)} author(s):**")
                
                # Show a collapsed preview list
                with st.expander("Show List"):
                    for a in author_list:
                        st.text(f"- {a}")

                # Preview the content for the FIRST author just as a sample
                sample_author = author_list[0]
                st.caption(f"Previewing generated code for sample: **{sample_author}**")
                
                use_dyn = (content_mode == "Dynamic (Lua Module)")
                
                # 1. Author Page Preview
                st.markdown(f"**1. Author:{sample_author}**")
                txt_author = format_author_page(sample_author, book_title, book_year, use_dyn)
                st.code(txt_author, language="mediawiki")

                # 2. Main Category Preview
                st.markdown(f"**2. Category:{sample_author}**")
                txt_cat_main = format_author_cat_page(sample_author)
                st.code(txt_cat_main, language="mediawiki")

                # 3. Works Category Preview
                cat_works_title = f"Category:Text_of_works_by_{sample_author.replace(' ', '_')}"
                st.markdown(f"**3. {cat_works_title}**")
                txt_cat_works = format_works_cat_page(sample_author)
                st.code(txt_cat_works, language="mediawiki")

                st.divider()

                # Calculate totals for clarity
                num_authors = len(author_list)
                total_wikipages = num_authors * 3
                
                if st.button(f"üöÄ Process {num_authors} Author(s) (Creates {total_wikipages} Wiki Pages)", type="primary"):
                    progress_bar = st.progress(0)
                    status_box = st.empty()
                    
                    total_ops = len(author_list)
                    success_count = 0
                    
                    for i, author_name in enumerate(author_list):
                        status_box.write(f"Processing **{author_name}** ({i+1}/{total_ops})...")
                        
                        # Prepare Content
                        p_author = f"Author:{author_name}"
                        p_cat_main = f"Category:{author_name}"
                        p_cat_works = f"Category:Text_of_works_by_{author_name.replace(' ', '_')}"
                        
                        txt_author = format_author_page(author_name, book_title, book_year, use_dyn)
                        txt_cat_main = format_author_cat_page(author_name)
                        txt_cat_works = format_works_cat_page(author_name)

                        try:
                            # --- A. Bahai.works Uploads ---
                            # 1. Author Page
                            upload_to_bahaiworks(p_author, txt_author, "Created Author page (Misc Tool)", check_exists=True)
                            
                            # 2. Main Category
                            upload_to_bahaiworks(p_cat_main, txt_cat_main, "Created Author Category", check_exists=True)
                            
                            # 3. Works Category
                            upload_to_bahaiworks(p_cat_works, txt_cat_works, "Created Works Category", check_exists=True)
                            
                            # --- B. Bahaidata Sync (NEW) ---
                            status_box.write(f"Syncing Bahaidata for **{author_name}**...")
                            qid, linked, link_msg = ensure_wikibase_author(author_name)
                            
                            if linked:
                                st.toast(f"‚úÖ {author_name}: Pages created & Linked to {qid}")
                            else:
                                st.warning(f"Pages created, but Link failed for {qid}: {link_msg}")
                            
                            success_count += 1

                        except FileExistsError:
                            # Even if pages exist, we should still try to ensure the Link exists!
                            try:
                                qid, linked, link_msg = ensure_wikibase_author(author_name)
                                if linked:
                                    st.toast(f"Updated Link for existing author: {qid}")
                            except Exception as e_link:
                                st.error(f"Link Error on {author_name}: {e_link}")
                                
                        except Exception as e:
                            st.error(f"‚ùå Error on {author_name}: {e}")
                        
                        progress_bar.progress((i + 1) / total_ops)

                    status_box.success(f"‚úÖ Process Complete! Processed {success_count} authors.")
                    st.balloons()

# --- TAB: AC MESSAGE ---
with tab_ac:
    st.header("AC Message")
    
    st.subheader("Copyright AC-Message Creator")
    st.info("Creates the /AC-Message subpage required for copyright-protected works.")
    
    col_b1, col_b2 = st.columns(2)
    
    with col_b1:
        bk_title = st.text_input("Book Title (Page Name)", placeholder="e.g. My_Book_Title")
        # Helper to suggest cover name if title is entered
        suggested_cover = f"{bk_title}.png" if bk_title else ""
        bk_cover = st.text_input("Cover Image Filename", value=suggested_cover, placeholder="e.g. My_Book_Title.png")
    
    with col_b2:
        if bk_title and bk_cover:
            target_page = f"{bk_title.strip()}/AC-Message"
            ac_content = format_ac_message(bk_title.strip(), bk_cover.strip())
            
            st.markdown(f"**Target:** `{target_page}`")
            st.code(ac_content, language="mediawiki")
            
            if st.button("üöÄ Create AC-Message Page", type="primary"):
                try:
                    upload_to_bahaiworks(
                        target_page, 
                        ac_content, 
                        "Created AC-Message (Misc Tool)", 
                        check_exists=True
                    )
                    st.success(f"‚úÖ Created {target_page}")
                except FileExistsError:
                    st.error(f"‚ö†Ô∏è Page '{target_page}' already exists.")
                except Exception as e:
                    st.error(f"Error: {e}")

# --- TAB: UPDATE AUTHOR PAGE ---
with tab_create_author_page:
    st.header("üîß Update Author list")
    
    st.subheader("Update 'Authors' Index Page")
    st.info("Scans categories Authors-A through Authors-Z, formats the list, and updates the [[Authors]] page.")
    
    if st.button("üîÑ Scan & Update [[Authors]]", type="primary"):
        status_box = st.empty()
        prog_bar = st.progress(0)
        
        full_wikitext = AUTHORS_PAGE_HEADER
        
        try:
            # Iterate A-Z
            letters = [chr(i) for i in range(ord('A'), ord('Z') + 1)]
            
            for i, letter in enumerate(letters):
                status_box.write(f"Scanning Category:Authors-{letter}...")
                
                # Fetch members via public API (faster, no auth needed for read)
                cat_url = "https://bahai.works/api.php"
                params = {
                    'action': 'query',
                    'list': 'categorymembers',
                    'cmtitle': f'Category:Authors-{letter}',
                    'format': 'json',
                    'cmlimit': 'max'
                }
                
                # Retrieve all pages in category
                members = []
                while True:
                    resp = requests.get(cat_url, params=params).json()
                    members.extend([p['title'] for p in resp.get('query', {}).get('categorymembers', [])])
                    if 'continue' not in resp:
                        break
                    params['cmcontinue'] = resp['continue']['cmcontinue']
                
                # Filter exclusions
                valid_members = [m for m in members if m not in EXCLUSION_LIST]
                
                # Only write header if we have content
                if valid_members:
                    full_wikitext += f"==== {letter} ====\n"
                    
                    for page_title in valid_members:
                        # Strip "Author:" prefix for the display name logic
                        clean_name = page_title.replace("Author:", "")
                        
                        # Use our robust parser
                        display_name = get_lastname_firstname(clean_name)
                        
                        full_wikitext += f"* [[{page_title}|{display_name}]]\n"
                    
                    full_wikitext += "\n\n"
                
                prog_bar.progress((i + 1) / 26)

            # Upload
            status_box.write("Uploading result...")
            
            # Using your existing uploader with overwrite protection DISABLED 
            # (because we WANT to update this specific index page)
            upload_to_bahaiworks(
                "Authors", 
                full_wikitext, 
                "Automated update of Authors index (Maintenance Tool)", 
                check_exists=False 
            )
            
            status_box.success("‚úÖ [[Authors]] page has been updated!")
            st.balloons()
            
            with st.expander("View Generated Source"):
                st.code(full_wikitext, language="mediawiki")

        except Exception as e:
            status_box.error(f"Error: {e}")

# ==============================================================================
# TAB 4: MAINTENANCE (AUDIT)
# ==============================================================================
with tab_maintenance:
    st.header("üîß Maintenance Audit")
    st.markdown("""
    **Goal:** Identify authors who have Chapters (P9) or Articles (P7) assigned in Bahaidata 
    but are missing the corresponding dynamic code on their Bahai.works author page.
    """)

    # Helper: SPARQL Query
    def query_bahaidata_authors():
        endpoint = "https://query.bahaidata.org/sparql"
        query = """
        PREFIX wdt: <http://www.wikidata.org/prop/direct/>
        PREFIX schema: <http://schema.org/>

        SELECT ?author ?authorLabel ?sitelink (MAX(?isChapter) as ?hasChapter) (MAX(?isArticle) as ?hasArticle) WHERE {
          {
            SELECT ?author (true as ?isChapter) (false as ?isArticle) WHERE {
              ?work wdt:P11 ?author ; wdt:P9 ?book .
            }
          } UNION {
            SELECT ?author (false as ?isChapter) (true as ?isArticle) WHERE {
              ?work wdt:P11 ?author ; wdt:P7 ?issue .
            }
          }
          
          OPTIONAL {
            ?sitelink schema:about ?author ;
                      schema:isPartOf <https://bahai.works/> .
          }
          
          SERVICE wikibase:label { bd:serviceParam wikibase:language "en". }
        }
        GROUP BY ?author ?authorLabel ?sitelink
        """
        try:
            r = requests.get(endpoint, params={'format': 'json', 'query': query})
            data = r.json()
            results = []
            for item in data['results']['bindings']:
                label = item['authorLabel']['value']
                url = item.get('sitelink', {}).get('value', None)
                has_chap = item.get('hasChapter', {}).get('value') == 'true'
                has_art = item.get('hasArticle', {}).get('value') == 'true'
                
                # Clean URL to get Title
                page_title = None
                if url:
                    page_title = url.replace("https://bahai.works/", "").replace("_", " ")
                    import urllib.parse
                    page_title = urllib.parse.unquote(page_title)

                results.append({
                    "Author": label,
                    "Page Title": page_title,
                    "Has Chapters": has_chap,
                    "Has Articles": has_art
                })
            return pd.DataFrame(results)
        except Exception as e:
            st.error(f"SPARQL Error: {e}")
            return pd.DataFrame()

    if st.button("üîé Run Audit (SPARQL + Content Check)", type="primary"):
        with st.spinner("1/2 Querying Bahaidata..."):
            df_audit = query_bahaidata_authors()
        
        if df_audit.empty:
            st.warning("No data returned or error occurred.")
        else:
            st.write(f"Found {len(df_audit)} candidates with publications.")
            
            report_data = []
            
            with st.spinner("2/2 Verifying Page Content on Bahai.works..."):
                # Prepare batch check
                pages_to_check = df_audit[df_audit["Page Title"].notna()]["Page Title"].unique().tolist()
                
                # Fetch content in chunks
                content_map = {}
                chunk_size = 50
                api_url = "https://bahai.works/api.php"
                
                for i in range(0, len(pages_to_check), chunk_size):
                    chunk = pages_to_check[i:i+chunk_size]
                    params = {
                        "action": "query",
                        "titles": "|".join(chunk),
                        "prop": "revisions",
                        "rvprop": "content",
                        "format": "json"
                    }
                    try:
                        r = requests.get(api_url, params=params).json()
                        pages = r.get("query", {}).get("pages", {})
                        for pid, pdata in pages.items():
                            title = pdata['title']
                            if "revisions" in pdata:
                                content_map[title] = pdata["revisions"][0]["*"]
                            else:
                                content_map[title] = "" 
                    except:
                        pass
            
            # Build Final Report
            for idx, row in df_audit.iterrows():
                p_title = row["Page Title"]
                status = "OK"
                issue_details = []
                
                # Check 1: Missing Page
                if not p_title:
                    status = "MISSING PAGE"
                    issue_details.append("No Bahai.works page linked")
                else:
                    # Check 2: Content
                    txt = content_map.get(p_title, "")
                    
                    if row["Has Chapters"] and "getChaptersByAuthor" not in txt:
                        status = "NEEDS UPDATE"
                        issue_details.append("Missing 'getChaptersByAuthor'")
                        
                    if row["Has Articles"] and "getArticlesByAuthor" not in txt:
                        status = "NEEDS UPDATE"
                        issue_details.append("Missing 'getArticlesByAuthor'")
                
                if status != "OK":
                    report_data.append({
                        "Author": row["Author"],
                        "Status": status,
                        "Issues": ", ".join(issue_details),
                        "Page": p_title if p_title else "N/A"
                    })
            
            # Display
            if not report_data:
                st.success("‚úÖ Amazing! All authors are perfectly synced.")
            else:
                final_df = pd.DataFrame(report_data)
                
                # Split display
                missing_pages = final_df[final_df["Status"] == "MISSING PAGE"]
                needs_update = final_df[final_df["Status"] == "NEEDS UPDATE"]
                
                st.subheader(f"‚ö†Ô∏è Issues Found: {len(final_df)}")
                
                tab_miss, tab_upd = st.tabs([f"Missing Pages ({len(missing_pages)})", f"Needs Code Update ({len(needs_update)})"])
                
                with tab_miss:
                    st.dataframe(missing_pages, use_container_width=True)
                    if not missing_pages.empty:
                        if st.button("Send to 'Create Author Pages'"):
                            st.session_state["batch_author_list"] = missing_pages["Author"].tolist()
                            st.switch_page("pages/06_misc_tasks.py") 
                
                with tab_upd:
                    st.dataframe(needs_update, use_container_width=True)
                    st.caption("These pages exist but need the Lua invoke code added.")
